For this course, we were asked to develop a concept integrating artificial intelligence, with particular emphasis on the use of AI agents as a mediating interface between humans and technology.

The project is situated within the musical domain. The initial idea involved creating an object that would allow users to improvise using only their fingers, while receiving real-time feedback from an AI agent. However, due to the strict time constraints of the project (two days) and current technological limitations—particularly the response time of AI agents—this approach proved unfeasible.

As a result, the concept was simplified while preserving the core idea of musical jamming. In the final version, the user generates a rhythmic pattern through a sequence of touches on a touch sensor; the AI agent analyzes the resulting rhythm and suggests a song with a similar rhythmic structure, providing a YouTube link.

The prototype was developed using a Raspberry Pi, a PC with Arduino, a breadboard, and a self-built touch sensor. In the absence of a commercial sensor, a custom touch sensor was constructed using a thin copper sheet, exploiting its electrical conductivity to detect user input. The main challenges emerged during the programming phase and the integration between hardware components and AI agents.
